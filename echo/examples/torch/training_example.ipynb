{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    #torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_fn = \"model_config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(conf_fn) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = conf[\"filter1\"]\n",
    "filter2 = conf[\"filter2\"]\n",
    "learning_rate = conf[\"learning_rate\"]\n",
    "batch_size = conf[\"batch_size\"]\n",
    "dropout = conf[\"dropout\"]\n",
    "epochs = conf[\"epochs\"]\n",
    "seed = conf[\"seed\"]\n",
    "\n",
    "stopping_patience = conf[\"early_stopping_patience\"]\n",
    "lr_patience = conf[\"lr_patience\"]\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()#tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Resize images for pytorch\n",
    "x_train = x_train.transpose((0, 3, 1, 2))\n",
    "x_test = x_test.transpose((0, 3, 1, 2))\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(x_train).float(), \n",
    "    torch.from_numpy(y_train).long()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2)\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(x_test).float(), \n",
    "    torch.from_numpy(y_test).long()\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False, \n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, filter1, filter2, dropout, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, filter1, 3)\n",
    "        self.conv2 = nn.Conv2d(filter1, filter2, 3)\n",
    "        self.fc1 = nn.Linear(4 * filter2 * 3 * 3, num_classes)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dr1 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dr1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(filter1, filter2, dropout, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    ")\n",
    "\n",
    "### Load loss\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "### Load schedulers \n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience = lr_patience, \n",
    "    verbose = verbose,\n",
    "    min_lr = 1.0e-13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_acc 0.43008237364043506 valid_acc 0.5191693290734825\n",
      "Epoch 1 train_acc 0.5302103326935381 valid_acc 0.5729832268370607\n",
      "Epoch 2 train_acc 0.568937939859245 valid_acc 0.5976437699680511\n",
      "Epoch 3 train_acc 0.5946497120921305 valid_acc 0.6108226837060703\n",
      "Epoch 4 train_acc 0.6121241202815099 valid_acc 0.6317891373801917\n",
      "Epoch 5 train_acc 0.6270193538067819 valid_acc 0.6282947284345048\n",
      "Epoch 6 train_acc 0.6353366922584773 valid_acc 0.6523562300319489\n",
      "Epoch 7 train_acc 0.6428142994241842 valid_acc 0.6471645367412141\n",
      "Epoch 8 train_acc 0.6479526551503519 valid_acc 0.6572484025559105\n",
      "Epoch 9 train_acc 0.6524112284069098 valid_acc 0.6644369009584664\n",
      "Epoch 10 train_acc 0.6590491042866283 valid_acc 0.6652356230031949\n",
      "Epoch 11 train_acc 0.6647672744721689 valid_acc 0.6591453674121406\n",
      "Epoch 12 train_acc 0.6664267434420985 valid_acc 0.6683306709265175\n",
      "Epoch 13 train_acc 0.6726847408829175 valid_acc 0.6613418530351438\n",
      "Epoch 14 train_acc 0.6754838451695457 valid_acc 0.6765175718849841\n",
      "Epoch 15 train_acc 0.6797824696097249 valid_acc 0.6814097444089456\n",
      "Epoch 16 train_acc 0.6830214331413947 valid_acc 0.6830071884984026\n",
      "Epoch 17 train_acc 0.686860204734485 valid_acc 0.6851038338658147\n",
      "Epoch 18 train_acc 0.6868801983365324 valid_acc 0.6785143769968051\n",
      "Epoch 19 train_acc 0.6886796225207934 valid_acc 0.6810103833865815\n",
      "Epoch 20 train_acc 0.6926383557261676 valid_acc 0.6904952076677316\n",
      "Epoch 21 train_acc 0.6968570057581573 valid_acc 0.6878993610223643\n",
      "Epoch 22 train_acc 0.696777031349968 valid_acc 0.6875998402555911\n",
      "Epoch 23 train_acc 0.6980366282789507 valid_acc 0.6906948881789138\n",
      "Epoch 24 train_acc 0.6994361804222649 valid_acc 0.6932907348242812\n",
      "Epoch 25 train_acc 0.7004758477287268 valid_acc 0.6956869009584664\n",
      "Epoch 26 train_acc 0.7023152591170825 valid_acc 0.6906948881789138\n",
      "Epoch 27 train_acc 0.7048544465770953 valid_acc 0.6933905750798722\n",
      "Epoch 28 train_acc 0.7025551823416507 valid_acc 0.6885982428115016\n",
      "Epoch 29 train_acc 0.7076135636596289 valid_acc 0.6928913738019169\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 30 train_acc 0.7276071657069738 valid_acc 0.7043730031948882\n",
      "Epoch 31 train_acc 0.7292066538707613 valid_acc 0.7045726837060703\n",
      "Epoch 32 train_acc 0.7310260716570698 valid_acc 0.7064696485623003\n",
      "Epoch 33 train_acc 0.7311660268714012 valid_acc 0.7061701277955271\n",
      "Epoch 34 train_acc 0.7270073576455535 valid_acc 0.7066693290734825\n",
      "Epoch 35 train_acc 0.7318658029430583 valid_acc 0.705870607028754\n",
      "Epoch 36 train_acc 0.7300463851567498 valid_acc 0.7060702875399361\n",
      "Epoch 37 train_acc 0.7317258477287268 valid_acc 0.7075678913738019\n",
      "Epoch 38 train_acc 0.730406269993602 valid_acc 0.7060702875399361\n",
      "Epoch 39 train_acc 0.7332053742802304 valid_acc 0.7051717252396166\n",
      "Epoch 40 train_acc 0.731505918106206 valid_acc 0.7074680511182109\n",
      "Epoch 41 train_acc 0.7339251439539347 valid_acc 0.7085662939297125\n",
      "Epoch 42 train_acc 0.7335852527191299 valid_acc 0.7096645367412141\n",
      "Epoch 43 train_acc 0.7332253678822777 valid_acc 0.7080670926517572\n",
      "Epoch 44 train_acc 0.7317658349328215 valid_acc 0.7085662939297125\n",
      "Epoch 45 train_acc 0.7338051823416507 valid_acc 0.7073682108626198\n",
      "Epoch 46 train_acc 0.733705214331414 valid_acc 0.7103634185303515\n",
      "Epoch 47 train_acc 0.7321657069737684 valid_acc 0.7063698083067093\n",
      "Epoch 48 train_acc 0.7324456174024312 valid_acc 0.7107627795527156\n",
      "Epoch 49 train_acc 0.7336652271273193 valid_acc 0.7091653354632588\n",
      "Epoch 50 train_acc 0.7329454574536148 valid_acc 0.7086661341853036\n",
      "Epoch 51 train_acc 0.7347248880358286 valid_acc 0.7074680511182109\n",
      "Epoch 52 train_acc 0.7339251439539347 valid_acc 0.7068690095846646\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 53 train_acc 0.7364243442098528 valid_acc 0.7097643769968051\n",
      "Epoch 54 train_acc 0.7366442738323736 valid_acc 0.7107627795527156\n",
      "Epoch 55 train_acc 0.7352647152911068 valid_acc 0.709564696485623\n",
      "Epoch 56 train_acc 0.7350647792706334 valid_acc 0.7101637380191693\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 57 train_acc 0.7352247280870121 valid_acc 0.7105630990415336\n",
      "Epoch 58 train_acc 0.7355846129238643 valid_acc 0.7104632587859425\n"
     ]
    }
   ],
   "source": [
    "results_dict = defaultdict(list)\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    \"\"\" Train \"\"\"\n",
    "    train_loss, train_accuracy = [], []\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss.append(loss.item())\n",
    "        train_accuracy.append(\n",
    "            (torch.argmax(outputs, -1) == labels.squeeze(-1)).float().mean().item()\n",
    "        )\n",
    "        \n",
    "    \"\"\" Validate \"\"\"\n",
    "    val_loss, val_accuracy = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device) \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze(-1))\n",
    "\n",
    "            # print statistics\n",
    "            val_loss.append(loss.item())\n",
    "            val_accuracy.append(\n",
    "                (torch.argmax(outputs, -1) == labels.squeeze(-1)).float().mean().item()\n",
    "            )\n",
    "    \n",
    "    results_dict[\"train_loss\"].append(np.mean(train_loss))\n",
    "    results_dict[\"train_accuracy\"].append(np.mean(train_accuracy))\n",
    "    results_dict[\"valid_loss\"].append(np.mean(val_loss))\n",
    "    results_dict[\"valid_accuracy\"].append(np.mean(val_accuracy))\n",
    "    \n",
    "    print(f'Epoch {epoch} train_acc {results_dict[\"train_accuracy\"][-1]} valid_acc {results_dict[\"valid_accuracy\"][-1]}')\n",
    "    \n",
    "    # Anneal learning rate\n",
    "    lr_scheduler.step(1-results_dict[\"valid_accuracy\"][-1])\n",
    "    \n",
    "    # Early stopping\n",
    "    best_epoch = [\n",
    "        i for i,j in enumerate(results_dict[\"valid_accuracy\"]) if j == max(results_dict[\"valid_accuracy\"])\n",
    "    ][0]\n",
    "    offset = epoch - best_epoch\n",
    "    if offset >= stopping_patience:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
